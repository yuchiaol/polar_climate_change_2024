{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(python_by_example)=\n",
    "\n",
    "# CNN Demonstration\n",
    "\n",
    "### The Model\n",
    "\n",
    "We train CNN model to predict nino3.4 index\n",
    "\n",
    "### Running the CNN Model\n",
    "\n",
    "1. Set up python packages including xarray, sklearn, and torch.\n",
    "2. Put these files under the same folder:\n",
    "   - enso_prediction_mlo.py \n",
    "   - sst.mon.mean.trefadj.anom.1880to2018.nc\n",
    "   - nino34.long.anom.data.txt\n",
    "\n",
    "### Python Script\n",
    "\n",
    "Import packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "#import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_enso_indices():\n",
    "  \"\"\"\n",
    "  Reads in the txt data file to output a pandas Series of ENSO vals\n",
    "\n",
    "  outputs\n",
    "  -------\n",
    "\n",
    "    pd.Series : monthly ENSO values starting from 1870-01-01\n",
    "  \"\"\"\n",
    "  with open('nino34.long.anom.data.txt') as f:\n",
    "    line = f.readline()\n",
    "    enso_vals = []\n",
    "    while line:\n",
    "        yearly_enso_vals = map(float, line.split()[1:])\n",
    "        enso_vals.extend(yearly_enso_vals)\n",
    "        line = f.readline()\n",
    "\n",
    "  enso_vals = pd.Series(enso_vals)\n",
    "  enso_vals.index = pd.date_range('1870-01-01',freq='MS',\n",
    "                                  periods=len(enso_vals))\n",
    "  enso_vals.index = pd.to_datetime(enso_vals.index)\n",
    "  return enso_vals\n",
    "\n",
    "def assemble_predictors_predictands(start_date, end_date, lead_time,\n",
    "                                    dataset, data_format,\n",
    "                                    num_input_time_steps=1,\n",
    "                                    use_pca=False, n_components=32,\n",
    "                                    lat_slice=None, lon_slice=None):\n",
    "  \"\"\"\n",
    "  inputs\n",
    "  ------\n",
    "\n",
    "      start_date           str : the start date from which to extract sst\n",
    "      end_date             str : the end date \n",
    "      lead_time            str : the number of months between each sst\n",
    "                              value and the target Nino3.4 Index\n",
    "      dataset              str : 'observations' 'CNRM' or 'MPI'\n",
    "      data_format          str : 'spatial' or 'flatten'. 'spatial' preserves\n",
    "                                  the lat/lon dimensions and returns an \n",
    "                                  array of shape (num_samples, num_input_time_steps,\n",
    "                                  lat, lon).  'flatten' returns an array of shape\n",
    "                                  (num_samples, num_input_time_steps*lat*lon)\n",
    "      num_input_time_steps int : the number of time steps to use for each \n",
    "                                 predictor sample\n",
    "      use_pca             bool : whether or not to apply principal components\n",
    "                              analysis to the sst field\n",
    "      n_components         int : the number of components to use for PCA\n",
    "      lat_slice           slice: the slice of latitudes to use \n",
    "      lon_slice           slice: the slice of longitudes to use\n",
    "\n",
    "  outputs\n",
    "  -------\n",
    "      Returns a tuple of the predictors (np array of sst temperature anomalies) \n",
    "      and the predictands (np array the ENSO index at the specified lead time).\n",
    "\n",
    "  \"\"\"\n",
    "  file_name = {'observations' : 'sst.mon.mean.trefadj.anom.1880to2018.nc',\n",
    "               'CNRM'         : 'CNRM_tas_anomalies_regridded.nc',\n",
    "               'MPI'          : 'MPI_tas_anomalies_regridded.nc'}[dataset]\n",
    "  variable_name = {'observations' : 'sst',\n",
    "                   'CNRM'         : 'tas',\n",
    "                   'MPI'          : 'tas'}[dataset]\n",
    "  ds = xr.open_dataset(file_name)\n",
    "  sst = ds[variable_name].sel(time=slice(start_date, end_date))\n",
    "  if lat_slice is not None:\n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "    raise NotImplementedError(\"In EXERCISE 7, you must implement the slicing!\")\n",
    "  if lon_slice is not None:\n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "    raise NotImplementedError(\"In EXERCISE 7, you must implement the slicing!\")\n",
    "\n",
    "\n",
    "  num_samples = sst.shape[0]\n",
    "  #sst is a (num_samples, lat, lon) array\n",
    "  #the line below converts it to (num_samples, num_input_time_steps, lat, lon)\n",
    "  sst = np.stack([sst.values[n-num_input_time_steps:n] for n in range(num_input_time_steps,\n",
    "                                                              num_samples+1)])\n",
    "  #CHALLENGE: CAN YOU IMPLEMENT THE ABOVE LINE WITHOUT A FOR LOOP?\n",
    "  num_samples = sst.shape[0]\n",
    "\n",
    "  sst[np.isnan(sst)] = 0\n",
    "  if data_format=='flatten':\n",
    "    #sst is a 3D array: (time_steps, lat, lon)\n",
    "    #in this tutorial, we will not be using ML models that take\n",
    "    #advantage of the spatial nature of global temperature\n",
    "    #therefore, we reshape sst into a 2D array: (time_steps, lat*lon)\n",
    "    #(At each time step, there are lat*lon predictors)\n",
    "    sst = sst.reshape(num_samples, -1)\n",
    "\n",
    "    #Use Principal Components Analysis, also called\n",
    "    #Empirical Orthogonal Functions, to reduce the\n",
    "    #dimensionality of the array\n",
    "    if use_pca:\n",
    "      pca = sklearn.decomposition.PCA(n_components=n_components)\n",
    "      pca.fit(sst)\n",
    "      X = pca.transform(sst)\n",
    "    else:\n",
    "      X = sst\n",
    "  else: # data_format=='spatial'\n",
    "    X = sst\n",
    "  start_date_plus_lead = pd.to_datetime(start_date) + \\\n",
    "                        pd.DateOffset(months=lead_time+num_input_time_steps-1)\n",
    "  end_date_plus_lead = pd.to_datetime(end_date) + \\\n",
    "                      pd.DateOffset(months=lead_time)\n",
    "  if dataset == 'observations':\n",
    "    y = load_enso_indices()[slice(start_date_plus_lead,\n",
    "                                  end_date_plus_lead)]\n",
    "  else: #the data is from a GCM\n",
    "    X = X.astype(np.float32)\n",
    "    #The Nino3.4 Index is composed of three month rolling values\n",
    "    #Therefore, when calculating the Nino3.4 Index in a GCM\n",
    "    #we have to extract the two months prior to the first target start date\n",
    "    target_start_date_with_2_month = start_date_plus_lead - pd.DateOffset(months=2)\n",
    "    subsetted_ds = ds[variable_name].sel(time=slice(target_start_date_with_2_month,\n",
    "                                                   end_date_plus_lead))\n",
    "    #Calculate the Nino3.4 index\n",
    "    y = subsetted_ds.sel(lat=slice(5,-5), lon=slice(360-170,360-120)).mean(dim=('lat','lon'))\n",
    "\n",
    "    y = pd.Series(y.values).rolling(window=3).mean()[2:].values\n",
    "    y = y.astype(np.float32)\n",
    "  ds.close()\n",
    "  return X.astype(np.float32), y.astype(np.float32)\n",
    "\n",
    "def plot_nino_time_series(y, predictions, title):\n",
    "  \"\"\"\n",
    "  inputs\n",
    "  ------\n",
    "    y           pd.Series : time series of the true Nino index\n",
    "    predictions np.array  : time series of the predicted Nino index (same\n",
    "                            length and time as y)\n",
    "    titile                : the title of the plot\n",
    "\n",
    "  outputs\n",
    "  -------\n",
    "    None.  Displays the plot\n",
    "  \"\"\"\n",
    "  predictions = pd.Series(predictions, index=y.index)\n",
    "  predictions = predictions.sort_index()\n",
    "  y = y.sort_index()\n",
    "\n",
    "  plt.plot(y, label='Ground Truth')\n",
    "  plt.plot(predictions, '--', label='ML Predictions')\n",
    "  plt.legend(loc='best')\n",
    "  plt.title(title)\n",
    "  plt.ylabel('Nino3.4 Index')\n",
    "  plt.xlabel('Date')\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "class ENSODataset(Dataset):\n",
    "    def __init__(self, predictors, predictands):\n",
    "        self.predictors = predictors\n",
    "        self.predictands = predictands\n",
    "        assert self.predictors.shape[0] == self.predictands.shape[0], \\\n",
    "               \"The number of predictors must equal the number of predictands!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.predictors.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.predictors[idx], self.predictands[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_input_time_steps=1, print_feature_dimension=False):\n",
    "        \"\"\"\n",
    "        inputs\n",
    "        -------\n",
    "            num_input_time_steps        (int) : the number of input time\n",
    "                                                steps in the predictor\n",
    "            print_feature_dimension    (bool) : whether or not to print\n",
    "                                                out the dimension of the features\n",
    "                                                extracted from the conv layers\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_input_time_steps, 6, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.print_layer = Print()\n",
    "\n",
    "        #ATTENTION EXERCISE 9: print out the dimension of the extracted features from \n",
    "        #the conv layers for setting the dimension of the linear layer!\n",
    "        #Using the print_layer, we find that the dimensions are \n",
    "        #(batch_size, 16, 42, 87)\n",
    "        self.fc1 = nn.Linear(16 * 42 * 87, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)\n",
    "        self.print_feature_dimension = print_feature_dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        if self.print_feature_dimension:\n",
    "          x = self.print_layer(x)\n",
    "        x = x.view(-1, 16 * 42 * 87)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class Print(nn.Module):\n",
    "    \"\"\"\n",
    "    This class prints out the size of the features\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        print(x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net, criterion, optimizer, trainloader, testloader,\n",
    "                  experiment_name, num_epochs=40):\n",
    "  \"\"\"\n",
    "  inputs\n",
    "  ------\n",
    "\n",
    "      net               (nn.Module)   : the neural network architecture\n",
    "      criterion         (nn)          : the loss function (i.e. root mean squared error)\n",
    "      optimizer         (torch.optim) : the optimizer to use update the neural network \n",
    "                                        architecture to minimize the loss function\n",
    "      trainloader       (torch.utils.data.DataLoader): dataloader that loads the\n",
    "                                        predictors and predictands\n",
    "                                        for the train dataset\n",
    "      testloader        (torch.utils.data. DataLoader): dataloader that loads the\n",
    "                                        predictors and predictands\n",
    "                                        for the test dataset\n",
    "  outputs\n",
    "  -------\n",
    "      predictions (np.array), and saves the trained neural network as a .pt file\n",
    "  \"\"\"\n",
    "  device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "  net = net.to(device)\n",
    "  best_loss = np.infty\n",
    "  train_losses, test_losses = [], []\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    for mode, data_loader in [('train', trainloader), ('test', testloader)]:\n",
    "      #Set the model to train mode to allow its weights to be updated\n",
    "      #while training\n",
    "      if mode == 'train':\n",
    "        net.train()\n",
    "\n",
    "      #Set the model to eval model to prevent its weights from being updated\n",
    "      #while testing\n",
    "      elif mode == 'test':\n",
    "        net.eval()\n",
    "      running_loss = 0.0\n",
    "      for i, data in enumerate(data_loader):\n",
    "          # get a mini-batch of predictors and predictands\n",
    "          batch_predictors, batch_predictands = data\n",
    "          batch_predictands = batch_predictands.to(device)\n",
    "          batch_predictors = batch_predictors.to(device)\n",
    "\n",
    "          # zero the parameter gradients\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          #calculate the predictions of the current neural network\n",
    "          predictions = net(batch_predictors).squeeze()\n",
    "\n",
    "          #quantify the quality of the predictions using a\n",
    "          #loss function (aka criterion) that is differentiable\n",
    "          loss = criterion(predictions, batch_predictands)\n",
    "\n",
    "          if mode == 'train':\n",
    "            #the 'backward pass: calculates the gradients of each weight\n",
    "            #of the neural network with respect to the loss\n",
    "            loss.backward()\n",
    "\n",
    "            #the optimizer updates the weights of the neural network\n",
    "            #based on the gradients calculated above and the choice\n",
    "            #of optimization algorithm\n",
    "            optimizer.step()\n",
    "\n",
    "          #Save the model weights that have the best performance!\n",
    "\n",
    "\n",
    "          running_loss += loss.item()\n",
    "      if running_loss < best_loss and mode == 'test':\n",
    "          best_loss = running_loss\n",
    "          torch.save(net, '{}.pt'.format(experiment_name))\n",
    "      print('{} Set: Epoch {:02d}. loss: {:3f}'.format(mode, epoch+1, \\\n",
    "                                            running_loss/len(data_loader)))\n",
    "      if mode == 'train':\n",
    "          train_losses.append(running_loss/len(data_loader))\n",
    "      else:\n",
    "          test_losses.append(running_loss/len(data_loader))\n",
    "\n",
    "  net = torch.load('{}.pt'.format(experiment_name))\n",
    "  net.eval()\n",
    "  net.to(device)\n",
    "\n",
    "  #the remainder of this notebook calculates the predictions of the best\n",
    "  #saved model\n",
    "  predictions = np.asarray([])\n",
    "  for i, data in enumerate(testloader):\n",
    "    batch_predictors, batch_predictands = data\n",
    "    batch_predictands = batch_predictands.to(device)\n",
    "    batch_predictors = batch_predictors.to(device)\n",
    "\n",
    "    batch_predictions = net(batch_predictors).squeeze()\n",
    "    #Edge case: if there is 1 item in the batch, batch_predictions becomes a float\n",
    "    #not a Tensor. the if statement below converts it to a Tensor\n",
    "    #so that it is compatible with np.concatenate\n",
    "    if len(batch_predictions.size()) == 0:\n",
    "      batch_predictions = torch.Tensor([batch_predictions])\n",
    "    predictions = np.concatenate([predictions, batch_predictions.detach().cpu().numpy()])\n",
    "  return predictions, train_losses, test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main script begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assemble numpy arrays corresponding to predictors and predictands\n",
    "train_start_date = '1960-01-01'\n",
    "train_end_date = '2005-12-31'\n",
    "num_input_time_steps = 1\n",
    "lead_time = 1\n",
    "train_predictors, train_predictands = assemble_predictors_predictands(train_start_date,\n",
    "                      train_end_date, lead_time, 'observations', 'spatial', num_input_time_steps=num_input_time_steps)\n",
    "test_predictors, test_predictands = assemble_predictors_predictands('2007-01-01',\n",
    "                    '2017-12-31', lead_time, 'observations', 'spatial', num_input_time_steps=num_input_time_steps)\n",
    "\n",
    "#Convert the numpy ararys into ENSODataset, which is a subset of the \n",
    "#torch.utils.data.Dataset class.  This class is compatible with\n",
    "#the torch dataloader, which allows for data loading for a CNN\n",
    "train_dataset = ENSODataset(train_predictors, train_predictands)\n",
    "test_dataset = ENSODataset(test_predictors, test_predictands)\n",
    "\n",
    "#Create a torch.utils.data.DataLoader from the ENSODatasets() created earlier!\n",
    "#the similarity between the name DataLoader and Dataset in the pytorch API is unfortunate...\n",
    "trainloader = DataLoader(train_dataset, batch_size=10)\n",
    "testloader = DataLoader(test_dataset, batch_size=10)\n",
    "net = CNN(num_input_time_steps=num_input_time_steps)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "experiment_name = \"twolayerCNN_{}_{}\".format(train_start_date, train_end_date)\n",
    "predictions, train_losses, test_losses = train_network(net, nn.MSELoss(),\n",
    "                  optimizer, trainloader, testloader, experiment_name)\n",
    "\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Performance of {} Neural Network During Training'.format(experiment_name))\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "corr, _ = pearsonr(test_predictands, predictions)\n",
    "rmse = mean_squared_error(test_predictands, predictions) ** 0.5\n",
    "plot_nino_time_series(test_predictands, predictions, '{} Predictions. \\n Corr: {:3f}. \\n RMSE: {:3f}.'.format(experiment_name,\n",
    "                                                                      corr, rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "```{figure} /_static/lecture_specific/figures/cnn_validation.png\n",
    ":scale: 40%\n",
    "```\n",
    "### Predictied Nino3.4 Index\n",
    "```{figure} /_static/lecture_specific/figures/cnn_nino34.png\n",
    ":scale: 40%\n",
    "```\n",
    "\n",
    "### Courtesy\n",
    "This code was modified from Glenn Liu's NCAR Hackthorn project.\n",
    "\n",
    "### References\n",
    "- [MIT deep learning open course](http://introtodeeplearning.com/)\n",
    "- [Neural Networks and Deep Learning by Michael Nielsen](http://neuralnetworksanddeeplearning.com/index.html)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   10,
   31,
   57,
   59,
   221,
   224,
   269,
   272,
   370,
   373,
   413
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}